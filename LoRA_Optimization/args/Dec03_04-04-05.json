{
    "seed": 26,
    "train_data": "./original_dataset/train_self_original.json",
    "valid_data": "./original_dataset/valid_self_original.json",
    "model_name": "Qwen/Qwen2.5-1.5B-Instruct",
    "train_epochs": 3,
    "train_batch": 4,
    "prompt_test": "<|im_start|>system\nYou are an intelligent conversational AI. I’ve set up a persona profile for you. \nPlease generate a response based on the persona profile and the dialogue history. \nThe response you generate should be consistent with the persona profile I set for you and coherent with the dialogue history. \nMoreover, the richer the content you generate, the better.<|im_end|>\n<|im_start|>user\nThe dialogue history between you and a user is as follows:\n»User: hi , how are you today ?\n»You: i am good how are you\n»User: really well ! i just started working online today\n»You: that is awesome is it a new job\n»User: yes , it is a reviewing site for guitar players\n»You: that is cool do you like music\n»User: i do . in fact i used to travel the us playing with my band\n»You: cool what kind of music\n»User: mostly rock , some country just like my favorite band story so far\n»You: cool i am really into rap music\n»User: awesome . who is your favorite artist ?\nThe persona profile I set up for you is as follows (in first person):\n»1. i like rap music .\n»2. i have a beard .\n»3. i graduated high school .\n»4. i live on a farm .\nNow please generate a response to the user based on the dialogue history and the profile.<|im_end|>\n<|im_start|>assistant\ni like new emerging artists<|im_end|>\n",
    "log_step": 1,
    "save_step": 100,
    "torch_dtype": "bfloat16",
    "max_seq_length": 512,
    "eval_batch": 8,
    "grad_acc_step": 4,
    "optimizer": "paged_adamw_32bit",
    "learning_rate": 0.0001,
    "weight_decay": 0.01,
    "warmup_step": 0.1,
    "lr_scheduler_type": "cosine",
    "lora_rank": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "use_lora": 1,
    "lora_q": 1,
    "lora_k": 0,
    "lora_v": 0,
    "trained_model_dir": "./outputs/in_context_learning/short_prompt",
    "inference_batch": 32,
    "num_beam": 1,
    "do_sample": 0,
    "temperature": 0,
    "top_p": 0,
    "output_path": "no_persona_input_casual_output",
    "test_data": "nli_mapped_panda_sample"
}